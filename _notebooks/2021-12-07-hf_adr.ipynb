{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Adverse Event\n","> Adverse Event entity recognition\n","\n","- toc: true \n","- badges: true\n","- comments: true\n","- categories: [jupyter]\n","- image: images/chart-preview.png"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2021-12-07 15:15:03.342950: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"]}],"source":["from datasets import Dataset, ClassLabel, Sequence, load_dataset, load_metric\n","import numpy as np\n","import pandas as pd\n","from spacy import displacy\n","import transformers\n","from transformers import (AutoModelForTokenClassification, \n","                          AutoTokenizer, \n","                          DataCollatorForTokenClassification,\n","                          pipeline,\n","                          TrainingArguments, \n","                          Trainer)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration default-8accc35da3484983\n","                            "]},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/aswin/.cache/huggingface/datasets/json/default-8accc35da3484983/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n","Dataset json downloaded and prepared to /home/aswin/.cache/huggingface/datasets/json/default-8accc35da3484983/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":[]}],"source":["cons_dataset = load_dataset(\"json\", data_files=\"/home/aswin/Documents/hf_course_event_adr/ADR_XTRACTER/dataset.jsonl\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# no train-test provided, so we create our own\n","cons_dataset = cons_dataset[\"train\"].train_test_split()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'drug', 'effect', 'drug_indices_start', 'drug_indices_end', 'effect_indices_start', 'effect_indices_end'],\n","        num_rows: 3203\n","    })\n","    test: Dataset({\n","        features: ['text', 'drug', 'effect', 'drug_indices_start', 'drug_indices_end', 'effect_indices_start', 'effect_indices_end'],\n","        num_rows: 1068\n","    })\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["cons_dataset"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["label_list = ['O', 'B-DRUG', 'I-DRUG', 'B-EFFECT', 'I-EFFECT']\n","\n","custom_seq = Sequence(feature=ClassLabel(num_classes=5, \n","                                         names=label_list,\n","                                         names_file=None, id=None), length=-1, id=None)\n","\n","cons_dataset[\"train\"].features[\"ner_tags\"] = custom_seq\n","cons_dataset[\"test\"].features[\"ner_tags\"] = custom_seq"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["model_checkpoint = \"SpanBERT/spanbert-large-cased\""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 414/414 [00:00<00:00, 139kB/s]\n","Downloading: 100%|██████████| 208k/208k [00:00<00:00, 488kB/s]\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def generate_row_labels(row, verbose=False):\n","    \"\"\" Given a row from the consolidated `Ade_corpus_v2_drug_ade_relation` dataset, \n","    generates BIO tags for drug and effect entities. \n","    \n","    \"\"\"\n","\n","    text = row[\"text\"]\n","\n","    labels = []\n","    label = \"O\"\n","    prefix = \"\"\n","    \n","    # while iterating through tokens, increment to traverse all drug and effect spans\n","    drug_index = 0\n","    effect_index = 0\n","    \n","    tokens = tokenizer(text, return_offsets_mapping=True)\n","\n","    for n in range(len(tokens[\"input_ids\"])):\n","        offset_start, offset_end = tokens[\"offset_mapping\"][n]\n","\n","        # should only happen for [CLS] and [SEP]\n","        if offset_end - offset_start == 0:\n","            labels.append(-100)\n","            continue\n","        \n","        if drug_index < len(row[\"drug_indices_start\"]) and offset_start == row[\"drug_indices_start\"][drug_index]:\n","            label = \"DRUG\"\n","            prefix = \"B-\"\n","\n","        elif effect_index < len(row[\"effect_indices_start\"]) and offset_start == row[\"effect_indices_start\"][effect_index]:\n","            label = \"EFFECT\"\n","            prefix = \"B-\"\n","        \n","        labels.append(label_list.index(f\"{prefix}{label}\"))\n","            \n","        if drug_index < len(row[\"drug_indices_end\"]) and offset_end == row[\"drug_indices_end\"][drug_index]:\n","            label = \"O\"\n","            prefix = \"\"\n","            drug_index += 1\n","            \n","        elif effect_index < len(row[\"effect_indices_end\"]) and offset_end == row[\"effect_indices_end\"][effect_index]:\n","            label = \"O\"\n","            prefix = \"\"\n","            effect_index += 1\n","\n","        # need to transition \"inside\" if we just entered an entity\n","        if prefix == \"B-\":\n","            prefix = \"I-\"\n","    \n","    if verbose:\n","        print(f\"{row}\\n\")\n","        orig = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n","        for n in range(len(labels)):\n","            print(orig[n], labels[n])\n","    tokens[\"labels\"] = labels\n","    \n","    return tokens"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': 'Ampicillin-associated seizures.', 'drug': ['Ampicillin'], 'effect': ['seizures'], 'drug_indices_start': [0], 'drug_indices_end': [10], 'effect_indices_start': [22], 'effect_indices_end': [30]}\n","\n","[CLS] -100\n","am 1\n","##pic 2\n","##ill 2\n","##in 2\n","- 0\n","associated 0\n","seizure 3\n","##s 4\n",". 0\n","[SEP] -100\n"]},{"data":{"text/plain":["{'input_ids': [101, 1821, 20437, 7956, 1394, 118, 2628, 20752, 1116, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (2, 5), (5, 8), (8, 10), (10, 11), (11, 21), (22, 29), (29, 30), (30, 31), (0, 0)], 'labels': [-100, 1, 2, 2, 2, 0, 0, 3, 4, 0, -100]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["generate_row_labels(cons_dataset[\"train\"][2], verbose=True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3203/3203 [00:06<00:00, 478.94ex/s]\n","100%|██████████| 1068/1068 [00:01<00:00, 979.60ex/s]\n"]}],"source":["labeled_dataset = cons_dataset.map(generate_row_labels)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'drug', 'drug_indices_end', 'drug_indices_start', 'effect', 'effect_indices_end', 'effect_indices_start', 'input_ids', 'labels', 'offset_mapping', 'text', 'token_type_ids'],\n","        num_rows: 3203\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'drug', 'drug_indices_end', 'drug_indices_start', 'effect', 'effect_indices_end', 'effect_indices_start', 'input_ids', 'labels', 'offset_mapping', 'text', 'token_type_ids'],\n","        num_rows: 1068\n","    })\n","})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["labeled_dataset"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n","batch_size = 16"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 634M/634M [00:57<00:00, 11.6MB/s]\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-{task}\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=5,\n","    weight_decay=0.05,\n","    logging_steps=1\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["metric = load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=labeled_dataset[\"train\"],\n","    eval_dataset=labeled_dataset[\"test\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics, \n","\n",")"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
